{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def loss(self, batch, outputs):\n",
    "        x, y = batch\n",
    "        x_recon, mean, log_var  = outputs\n",
    "\n",
    "        BCE = F.binary_cross_entropy(x_recon, x.view(-1, 784), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        \n",
    "        loss = BCE + KLD\n",
    "\n",
    "        return { 'loss': loss, 'BCE_loss': BCE, 'KLD_loss': KLD}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "int too large to convert to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m values_of_component_2\u001b[39m.\u001b[39mappend(kld)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m current_epoch \u001b[39m%\u001b[39m epochs_to_make_updates \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Change 3: Update weights of components\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     adapt_weights \u001b[39m=\u001b[39m softadapt_object\u001b[39m.\u001b[39;49mget_component_weights(torch\u001b[39m.\u001b[39;49mTensor(values_of_component_1), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m                                                          torch\u001b[39m.\u001b[39;49mTensor(values_of_component_2),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m                                                          verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m                                                          )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# Resetting the lists to start fresh (this part is optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxxxxx/Home/siv34/yyyyyy/projects/MastersThesis/src_2/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     values_of_component_1 \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/softadapt/algorithms/loss_weighted_variant.py:65\u001b[0m, in \u001b[0;36mLossWeightedSoftAdapt.get_component_weights\u001b[0;34m(self, verbose, *loss_component_values)\u001b[0m\n\u001b[1;32m     60\u001b[0m average_loss_values \u001b[39m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m loss_points \u001b[39min\u001b[39;00m loss_component_values:\n\u001b[1;32m     63\u001b[0m     \u001b[39m# Compute the rates of change for each one of the loss components.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     rates_of_change\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 65\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_rates_of_change(loss_points,\n\u001b[1;32m     66\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccuracy_order,\n\u001b[1;32m     67\u001b[0m                                       verbose\u001b[39m=\u001b[39;49mverbose))\n\u001b[1;32m     68\u001b[0m     average_loss_values\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mmean(loss_points\u001b[39m.\u001b[39mfloat()))\n\u001b[1;32m     70\u001b[0m rates_of_change \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(rates_of_change)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/softadapt/base/_softadapt_base_class.py:81\u001b[0m, in \u001b[0;36mSoftAdaptBase._compute_rates_of_change\u001b[0;34m(self, input_tensor, order, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_rates_of_change\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     59\u001b[0m                              input_tensor:torch\u001b[39m.\u001b[39mtensor,\n\u001b[1;32m     60\u001b[0m                              order: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m     61\u001b[0m                              verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Base class method for computing loss functions rate of change.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_finite_difference(input_array \u001b[39m=\u001b[39;49m input_tensor\u001b[39m.\u001b[39;49mnumpy(),\n\u001b[1;32m     82\u001b[0m                                   order \u001b[39m=\u001b[39;49m order,\n\u001b[1;32m     83\u001b[0m                                   verbose \u001b[39m=\u001b[39;49m verbose)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/softadapt/utilities/_finite_difference.py:70\u001b[0m, in \u001b[0;36m_get_finite_difference\u001b[0;34m(input_array, order, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy orders larger than 5 must be even. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mcheck the arguments passed to the function.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m order_is_even:\n\u001b[0;32m---> 70\u001b[0m     constants \u001b[39m=\u001b[39m coefficients(deriv\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, acc\u001b[39m=\u001b[39;49morder)[\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcoefficients\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m order \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/findiff/coefs.py:72\u001b[0m, in \u001b[0;36mcoefficients\u001b[0;34m(deriv, acc, offsets, symbolic, analytic_inv)\u001b[0m\n\u001b[1;32m     69\u001b[0m num_side \u001b[39m=\u001b[39m num_central \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     70\u001b[0m offsets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39mnum_side, num_side\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 72\u001b[0m ret[\u001b[39m\"\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m calc_coefs(deriv, offsets, symbolic, analytic_inv)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Determine forward coefficients\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m deriv \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/findiff/coefs.py:146\u001b[0m, in \u001b[0;36mcalc_coefs\u001b[0;34m(deriv, offsets, symbolic, analytic_inv)\u001b[0m\n\u001b[1;32m    144\u001b[0m     coefs \u001b[39m=\u001b[39m compute_inverse_Vandermonde(deriv, offsets, symbolic)\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     matrix \u001b[39m=\u001b[39m _build_matrix(offsets, symbolic)\n\u001b[1;32m    147\u001b[0m     rhs \u001b[39m=\u001b[39m _build_rhs(offsets, deriv, symbolic)\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m symbolic:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch2.1/lib/python3.10/site-packages/findiff/coefs.py:237\u001b[0m, in \u001b[0;36m_build_matrix\u001b[0;34m(offsets, symbolic)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m sympy\u001b[39m.\u001b[39mMatrix(A)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(A,dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mOverflowError\u001b[0m: int too large to convert to float"
     ]
    }
   ],
   "source": [
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "\n",
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
    "softadapt_object = LossWeightedSoftAdapt(beta=0.1)\n",
    "\n",
    "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
    "epochs_to_make_updates = 5\n",
    "\n",
    "values_of_component_1 = []\n",
    "values_of_component_2 = []\n",
    "# Initializing adaptive weights to all ones.\n",
    "adapt_weights = torch.tensor([1,1])\n",
    "\n",
    "for current_epoch in range(1, 10):\n",
    "    for x, y in mnist_dataloader:\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "\n",
    "        bce_loss = loss['BCE_loss']\n",
    "        kld = loss['KLD_loss']\n",
    "\n",
    "        values_of_component_1.append(bce_loss)\n",
    "        values_of_component_2.append(kld)\n",
    "\n",
    "        if current_epoch % epochs_to_make_updates == 0 and current_epoch > 0:\n",
    "            # Change 3: Update weights of components\n",
    "            adapt_weights = softadapt_object.get_component_weights(torch.Tensor(values_of_component_1), \n",
    "                                                                 torch.Tensor(values_of_component_2),\n",
    "                                                                 verbose=False,\n",
    "                                                                 )\n",
    "                                           \n",
    "        \n",
    "            # Resetting the lists to start fresh (this part is optional)\n",
    "            values_of_component_1 = []\n",
    "            values_of_component_2 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
